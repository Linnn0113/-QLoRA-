import streamlit as st
from openai import OpenAI

# é¡µé¢é…ç½®
st.set_page_config(page_title="åŒ»ç–—ä¸“å®¶è¯Šæ–­ç³»ç»Ÿ", page_icon="ğŸ©º")
st.title("ğŸ©º åŒ»ç–—æ€ç»´é“¾ä¸“å®¶ç³»ç»Ÿ")

# åˆå§‹åŒ–å®¢æˆ·ç«¯ (æŒ‡å‘æœ¬åœ° LLaMA Factory API)
client = OpenAI(api_key="0", base_url="http://localhost:8000/v1")

# ã€ä¸“å®¶çº§ç³»ç»Ÿæç¤ºè¯ã€‘
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸­åŒ»å¤–ç§‘æƒå¨ä¸“å®¶ã€‚åœ¨åˆ†æç—…ä¾‹æ—¶ï¼Œè¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ç©ºé—´å®šä½ï¼šå¤´çš®ç—…ç—‡åº”ä¼˜å…ˆè€ƒè™‘å¤–ç§‘ç–®ç–¡ï¼Œè€Œéå†…ç§‘ç–³ç§¯ã€‚
2. ç‰¹å¾åŒ¹é…ï¼šè‹¥ä½“å¾åŒ…å«â€œçš®ä¸‹ç©ºæ´â€ã€â€œçŠ¶å¦‚è¼è›„ç©¿æ˜â€ï¼Œè¿™æ˜¯â€œè¼è›„ç––â€çš„å”¯ä¸€é‡‘æ ‡å‡†ã€‚
3. é€»è¾‘ä¸¥å¯†ï¼šå¿…é¡»åœ¨<think>æ ‡ç­¾å†…è¿›è¡Œé‰´åˆ«è¯Šæ–­ï¼Œæ’é™¤æ‰ç›¸ä¼¼ä½†é”™è¯¯çš„ç—…åã€‚"""

# ã€é€»è¾‘çº åï¼šFew-shot å¼•å¯¼ã€‘
# å“ªæ€•æ¨¡å‹ç»ƒå¾—ä¸å¤Ÿæ·±ï¼Œè¿™ä¸¤ç»„å¯¹è¯ä¹Ÿèƒ½å¼ºè¡ŒæŠŠå®ƒçš„æ€ç»´å®šæ­»åœ¨æ­£ç¡®é€»è¾‘ä¸Š
FEW_SHOT_EXAMPLES = [
    {"role": "user", "content": "1å²æ‚£å„¿å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œæœ‰ç©ºæ´ï¼Œçš®è‚¤å¢åšã€‚è¯Šæ–­æ˜¯ä»€ä¹ˆï¼Ÿ"},
    {"role": "assistant", "content": "<think>ç—‡çŠ¶ç‚¹ï¼š1. å¹¼å„¿å¤å­£å‘ç—…ï¼›2. ä½åœ¨å¤´çš®ï¼›3. å…³é”®ä½“å¾ä¸ºçš®ä¸‹ç©ºæ´ã€‚æ’é™¤ï¼šç–³ç§¯æ— ç©¿æ˜æ€§ç©ºæ´ã€‚ç»“è®ºï¼šè¼è›„ç––ã€‚</think>æœ€ç»ˆè¯Šæ–­ï¼šè¼è›„ç––ã€‚"}
]

# åˆå§‹åŒ–ä¼šè¯å†å²
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    st.session_state.messages.extend(FEW_SHOT_EXAMPLES)

# æ˜¾ç¤ºå†å²ï¼ˆéšè—ç³»ç»Ÿæç¤ºå’Œç¤ºèŒƒï¼‰
for message in st.session_state.messages[3:]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if user_input := st.chat_input("è¯·è¾“å…¥ç—…ä¾‹æè¿°..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        # æ ¸å¿ƒå‚æ•°è®¾ç½®
        responses = client.chat.completions.create(
            model="DeepSeek-R1-Distill-Qwen-7B",
            messages=st.session_state.messages,
            stream=True,
            temperature=0.0,      # ã€å…³é”®ã€‘è®¾ä¸º 0 å½»åº•æ¶ˆé™¤éšæœºæ€§ï¼Œé˜²æ­¢å®ƒèƒ¡æ€ä¹±æƒ³
            max_tokens=600,       # é™åˆ¶é•¿åº¦é˜²æ­¢å¤è¯»
            presence_penalty=1.2, # æƒ©ç½šé‡å¤
            stop=["<ï½œendoftextï½œ>", "###"] 
        )

        for response in responses:
            token = response.choices[0].delta.content
            if token:
                full_response += token
                response_placeholder.markdown(full_response + "â–Œ")
        
        response_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})